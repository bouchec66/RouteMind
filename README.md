# ⚡ Energy-Aware LLM Chaining Framework

A modular, **energy-efficient framework** for building and experimenting with **plug-and-play LLM chains**. Designed for low-cost inference, flexible model selection, and rapid iteration across GPU or hosted environments like RunPod.

---

## 🚀 Project Goals

- ⚡ **Energy-aware orchestration**: dynamically route tasks to appropriate models based on cost/performance tradeoffs.
- 🧱 **Plug-and-play chain design**: swap in/out models, preprocessors, and logic with minimal friction.
- 🔄 **Reproducibility & Portability**: clean environment setup with rebuild/startup tooling.
- 🌐 **Remote-first**: developed with containerized or hosted GPU environments in mind.

---

## 📁 Project Structure


